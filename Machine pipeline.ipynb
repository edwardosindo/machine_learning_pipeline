{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "sc = SparkContext()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets initialize our sparksession now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"how to read csv file\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets then check the spark version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Batsman: string (nullable = true)\n",
      " |-- Batsman_Name: string (nullable = true)\n",
      " |-- Bowler: string (nullable = true)\n",
      " |-- Bowler_Name: string (nullable = true)\n",
      " |-- Commentary: string (nullable = true)\n",
      " |-- Detail: string (nullable = true)\n",
      " |-- Dismissed: string (nullable = true)\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Isball: string (nullable = true)\n",
      " |-- Isboundary: string (nullable = true)\n",
      " |-- Iswicket: string (nullable = true)\n",
      " |-- Over: string (nullable = true)\n",
      " |-- Runs: string (nullable = true)\n",
      " |-- Timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read csv\n",
    "df = spark.read.csv('ind-ban-comment.csv', header=True)\n",
    "\n",
    "#see the default Schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the Schema ---\n",
    "Now, we do not want all the columns in our dataset to be treated as strings. So what can we do about that?\n",
    "\n",
    "We can define the custom schema for our dataframe in Spark. For this, we need to create an object of StructType which takes a list of StructField. And of course, we should define StructField with a column name, the data type of the column and whether null values are allowed for the particular column or not.\n",
    "\n",
    "Refer to the below code snippet to understand how to create this custom schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Batsman: integer (nullable = true)\n",
      " |-- Batsman_Name: string (nullable = true)\n",
      " |-- Bowler: integer (nullable = true)\n",
      " |-- Bowler_Name: string (nullable = true)\n",
      " |-- Commentary: string (nullable = true)\n",
      " |-- Detail: string (nullable = true)\n",
      " |-- Dismissed: integer (nullable = true)\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- Isball: boolean (nullable = true)\n",
      " |-- Isboundary: integer (nullable = true)\n",
      " |-- Iswicket: integer (nullable = true)\n",
      " |-- Over: double (nullable = true)\n",
      " |-- Runs: integer (nullable = true)\n",
      " |-- Timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.types as tp\n",
    "\n",
    "#Define the schema\n",
    "my_schema = tp.StructType([\n",
    "    tp.StructField(name = 'Batsman', dataType= tp.IntegerType(), nullable= True),\n",
    "    tp.StructField( name = 'Batsman_Name', dataType= tp.StringType(), nullable= True),\n",
    "    tp.StructField(name = 'Bowler', dataType= tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name= 'Bowler_Name', dataType= tp.StringType(), nullable=True),\n",
    "    tp.StructField(name= 'Commentary', dataType= tp.StringType(), nullable=True),\n",
    "    tp.StructField(name= 'Detail', dataType= tp.StringType(), nullable=True),\n",
    "    tp.StructField(name= 'Dismissed', dataType= tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='Id', dataType= tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='Isball', dataType= tp.BooleanType(), nullable=True),\n",
    "    tp.StructField(name='Isboundary', dataType= tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name= 'Iswicket', dataType= tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name= 'Over', dataType= tp.DoubleType(), nullable=True),\n",
    "    tp.StructField(name= 'Runs', dataType= tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name= 'Timestamp', dataType= tp.TimestampType(), nullable=True)\n",
    "])\n",
    "\n",
    "#Read the Data again with the defined schema\n",
    "my_data = spark.read.csv('ind-ban-comment.csv', schema= my_schema, header=True,)\n",
    "\n",
    "#Print the Schema\n",
    "my_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Columns from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any machine learning project, we always have a few columns that are not required for solving problem. I'm sure you've come across this dilemma before as well, whether thats in the industry or in an online hackathon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In out instance, we can use the drop function to remove the column from the data. Use the asterisk(*) sign before the list to drop multiple columns from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Batsman_Name',\n",
       " 'Bowler_Name',\n",
       " 'Commentary',\n",
       " 'Detail',\n",
       " 'Dismissed',\n",
       " 'Isball',\n",
       " 'Isboundary',\n",
       " 'Iswicket',\n",
       " 'Over',\n",
       " 'Runs',\n",
       " 'Timestamp']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop the columns that are not required\n",
    "my_data = my_data.drop(*['Batsman', 'Bowler', 'Id'])\n",
    "my_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration using PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the Data Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike Pandas, Spark dataframes do not have the shape function to check the dimensions of the data. We can instead use the code below to check the dimensions of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(605, 11)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the dimensions of the data\n",
    "(my_data.count(), len(my_data.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparks describe function gives us most of the statistical results like mean, count, min, max, and standard deviation. You can use the summary function to get the quartiles of the numeric variables as well;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------------+\n",
      "|summary|Isboundary|              Runs|\n",
      "+-------+----------+------------------+\n",
      "|  count|        67|               605|\n",
      "|   mean|       1.0|0.9917355371900827|\n",
      "| stddev|       0.0| 1.342725481259329|\n",
      "|    min|         1|                 0|\n",
      "|    max|         1|                 6|\n",
      "+-------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get the summary of the numerical columns\n",
    "my_data.select('Isball', 'Isboundary', 'Runs').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing Values Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its rare when we get a dataset without any values. Can you remember the last time that happened?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to check the number of missing values present in all the columns. Knowing the count helps us treat the missing values before building any machine learning model using that data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you can use the code below to find the null value count in your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+----------+------+---------+------+----------+--------+----+----+---------+\n",
      "|Batsman_Name|Bowler_Name|Commentary|Detail|Dismissed|Isball|Isboundary|Iswicket|Over|Runs|Timestamp|\n",
      "+------------+-----------+----------+------+---------+------+----------+--------+----+----+---------+\n",
      "|           0|          0|         0|   565|      586|     0|       538|     586|   0|   0|        0|\n",
      "+------------+-----------+----------+------+---------+------+----------+--------+----+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import sql function pyspark\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "#null values in each column\n",
    "data_agg = my_data.agg(*[f.count(f.when(f.isnull(c),c)).alias(c) for c in my_data.columns])\n",
    "data_agg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value Counts of a Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike Pandas, we do not have the value_counts() function in Spark dataframes. You can use the groupBy function to calculate the unique value counts of categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|      Batsman_Name|count|\n",
      "+------------------+-----+\n",
      "|     Soumya Sarkar|   39|\n",
      "|  Mashrafe Mortaza|    5|\n",
      "|   Shakib Al Hasan|   75|\n",
      "|   Mushfiqur Rahim|   23|\n",
      "|Mohammad Saifuddin|   42|\n",
      "|         Liton Das|   24|\n",
      "|      Rishabh Pant|   43|\n",
      "|    Mohammed Shami|    2|\n",
      "|       Tamim Iqbal|   31|\n",
      "|     Hardik Pandya|    2|\n",
      "|          KL Rahul|   93|\n",
      "| Bhuvneshwar Kumar|    4|\n",
      "|     Rubel Hossain|   11|\n",
      "|      Rohit Sharma|   94|\n",
      "|    Dinesh Karthik|    9|\n",
      "|       Virat Kohli|   27|\n",
      "|          MS Dhoni|   33|\n",
      "|     Sabbir Rahman|   40|\n",
      "|  Mosaddek Hossain|    7|\n",
      "| Mustafizur Rahman|    1|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Value counts of Batsman_Name column\n",
    "my_data.groupBy('Batsman_Name').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode Categorical Variables using PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most machine learning algorithms accept the data only in numerical form. So, it is essential to convert any categorical variable present in our dataset into numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we cannot simply drop them from our dataset as they might contain useful information. It would be a nightmare to lose that just because we dont want to figure out how to use them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see some of the methods to encode categorical variables using PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "String Indexing is similar  to Label Encoding. It assigns a unique integer value to each category. 0 is assigned to the most frequent category, 1 to the next most frequent value, and so on. We have to define the input column name that we want to index and the output column name in which we want the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+------------------+------------+\n",
      "|     Batsman_Name|Batsman_Index|       Bowler_Name|Bowler_Index|\n",
      "+-----------------+-------------+------------------+------------+\n",
      "|   Mohammed Shami|         18.0| Mustafizur Rahman|         0.0|\n",
      "|Bhuvneshwar Kumar|         16.0| Mustafizur Rahman|         0.0|\n",
      "|   Mohammed Shami|         18.0| Mustafizur Rahman|         0.0|\n",
      "|Bhuvneshwar Kumar|         16.0| Mustafizur Rahman|         0.0|\n",
      "|         MS Dhoni|          7.0| Mustafizur Rahman|         0.0|\n",
      "|         MS Dhoni|          7.0| Mustafizur Rahman|         0.0|\n",
      "|         MS Dhoni|          7.0| Mustafizur Rahman|         0.0|\n",
      "|         MS Dhoni|          7.0|Mohammad Saifuddin|         8.0|\n",
      "|         MS Dhoni|          7.0|Mohammad Saifuddin|         8.0|\n",
      "|         MS Dhoni|          7.0|Mohammad Saifuddin|         8.0|\n",
      "+-----------------+-------------+------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer,OneHotEncoder\n",
    "\n",
    "#create object of StringIndexer class and specify input and output column\n",
    "SI_batsman = StringIndexer(inputCol = 'Batsman_Name', outputCol='Batsman_Index')\n",
    "SI_bowler = StringIndexer(inputCol='Bowler_Name', outputCol='Bowler_Index')\n",
    "\n",
    "#transform the data\n",
    "my_data = SI_batsman.fit(my_data).transform(my_data)\n",
    "my_data = SI_bowler.fit(my_data).transform(my_data)\n",
    "\n",
    "#view the transformed data\n",
    "my_data.select('Batsman_Name', 'Batsman_Index', 'Bowler_Name', 'Bowler_Index').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding is a concept every data scientitst should know. I've relied on it multiple times when dealing with missing values. Its a lifesaver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the caveat - Spark's OneHotEncoder does not directly encode the categorical variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"First, we need to use the String Indexer to convert the variable into numerical form and then use OneHotEncoder to encode multiple columns of the dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It creates a Sparse Vector for each row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+---------------+------------------+------------+--------------+\n",
      "|     Batsman_Name|Batsman_Index|    Batsman_OHE|       Bowler_Name|Bowler_Index|    Bowler_OHE|\n",
      "+-----------------+-------------+---------------+------------------+------------+--------------+\n",
      "|   Mohammed Shami|         18.0|(19,[18],[1.0])| Mustafizur Rahman|         0.0|(11,[0],[1.0])|\n",
      "|Bhuvneshwar Kumar|         16.0|(19,[16],[1.0])| Mustafizur Rahman|         0.0|(11,[0],[1.0])|\n",
      "|   Mohammed Shami|         18.0|(19,[18],[1.0])| Mustafizur Rahman|         0.0|(11,[0],[1.0])|\n",
      "|Bhuvneshwar Kumar|         16.0|(19,[16],[1.0])| Mustafizur Rahman|         0.0|(11,[0],[1.0])|\n",
      "|         MS Dhoni|          7.0| (19,[7],[1.0])| Mustafizur Rahman|         0.0|(11,[0],[1.0])|\n",
      "|         MS Dhoni|          7.0| (19,[7],[1.0])| Mustafizur Rahman|         0.0|(11,[0],[1.0])|\n",
      "|         MS Dhoni|          7.0| (19,[7],[1.0])| Mustafizur Rahman|         0.0|(11,[0],[1.0])|\n",
      "|         MS Dhoni|          7.0| (19,[7],[1.0])|Mohammad Saifuddin|         8.0|(11,[8],[1.0])|\n",
      "|         MS Dhoni|          7.0| (19,[7],[1.0])|Mohammad Saifuddin|         8.0|(11,[8],[1.0])|\n",
      "|         MS Dhoni|          7.0| (19,[7],[1.0])|Mohammad Saifuddin|         8.0|(11,[8],[1.0])|\n",
      "+-----------------+-------------+---------------+------------------+------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create object and specify input and output column\n",
    "OHE = OneHotEncoder(inputCols=['Batsman_Index', 'Bowler_Index'], outputCols=['Batsman_OHE', 'Bowler_OHE'])\n",
    "\n",
    "#transform the data\n",
    "my_data = OHE.fit(my_data).transform(my_data)\n",
    "\n",
    "#view and transform the data\n",
    "my_data.select('Batsman_Name', 'Batsman_Index', 'Batsman_OHE', 'Bowler_Name', 'Bowler_Index', 'Bowler_OHE').show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Assembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"A. vector assembler combines a given list of columns into a single vector column\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is typically used at the end of the data exploration and pre-processing steps. At this stage, we usually work with a few raw or transformed features that can be used to train our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Vector Assembler converts them into a single feature column in order to train the machine learning model(such as Logistic Regression). It accepts numerical, boolean and vector type columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              vector|\n",
      "+--------------------+\n",
      "|(36,[1,2,4,24,25]...|\n",
      "|(36,[1,2,3,4,22,2...|\n",
      "|(36,[2,3,4,24,25]...|\n",
      "|(36,[2,3,4,22,25]...|\n",
      "|(36,[1,2,4,13,25]...|\n",
      "|(36,[2,4,13,25],[...|\n",
      "|(36,[2,4,13,25],[...|\n",
      "|(36,[2,3,4,5,13,3...|\n",
      "|(36,[0,2,3,4,5,13...|\n",
      "|(36,[2,4,5,13,33]...|\n",
      "|(36,[2,4,5,13,33]...|\n",
      "|(36,[0,2,3,4,5,13...|\n",
      "|(36,[2,3,4,5,13,3...|\n",
      "|(36,[2,4,22,25],[...|\n",
      "|(36,[2,3,4,13,25]...|\n",
      "|(36,[2,4,13,25],[...|\n",
      "|(36,[2,3,4,22,25]...|\n",
      "|(36,[1,2,4,19,25]...|\n",
      "|(36,[2,3,4,13,25]...|\n",
      "|(36,[2,3,4,5,13,3...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#specify the input and output columns of the vector assembler\n",
    "assembler = VectorAssembler(inputCols=['Isboundary', 'Iswicket', 'Over', 'Runs', 'Batsman_Index', 'Bowler_Index', 'Batsman_OHE', 'Bowler_OHE'], outputCol='vector')\n",
    "\n",
    "#fill the null values\n",
    "my_data = my_data.fillna(0)\n",
    "\n",
    "#transform the data\n",
    "final_data = assembler.transform(my_data)\n",
    "\n",
    "#view the transformed vector\n",
    "final_data.select('vector').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Machine Learning Pipelines using PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A machine learning project typically involves steps like data preprocessing, feature extraction, model fitting and evaluating results. We need to perform alot of transformations on the data in sequence. As you can imagine, keeping track of them can potentially become a tedious task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where machine learning pipeline come in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"A pipeline allows us to maintain the data flow of all the relevant transformations that are required to reach the end result.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the stages of the pipeline which act as a chain of command for spark to run. Here, each stage is either a Transformer or an Estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers and Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name suggests, Transformers convert one dataframe into another either by updating the current values of a particular column(like converting categorical columns to numeric) or mapping it to some other values by using a defined logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Estimator implements the fit()method on a dataframe and produces a model. For example, LogisticRegression is an Estimator that trains a classification model when we call the fit() method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets understand this with the help of some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a sample dataframe with three columns as shown below. Here, we will define some of the stages in which we want to transform the data and see how to set up the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+\n",
      "| id|category_1|category_2|\n",
      "+---+----------+----------+\n",
      "|  1|      L101|         R|\n",
      "|  2|      L201|         c|\n",
      "|  3|      D111|         R|\n",
      "|  4|      F210|         R|\n",
      "|  5|      D110|         C|\n",
      "+---+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#create a sample dataframe\n",
    "sample_df = spark.createDataFrame([\n",
    "    (1, 'L101', 'R'),\n",
    "    (2, 'L201', 'c'),\n",
    "    (3, 'D111', 'R'),\n",
    "    (4, 'F210', 'R'),\n",
    "    (5, 'D110', 'C')\n",
    "], ['id', 'category_1', 'category_2'])\n",
    "\n",
    "sample_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created the dataframe. Suppose we have to transform the data in the below order:\n",
    "1. stage_1: Label Encode or String Index the column category_1 \n",
    "2. stage_2: Label Encode or String Index the column category_2\n",
    "3. stage_3: One-Hot Encode the indexed column category_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each stage, we will pass the input and output column name and setup the pipeline by passing the defined stages in the list of the Pipeline object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline model then performs certain steps one by one in a sequence and gives us the end result. Lets see how to implement the pipeline:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----------------+----------------+--------------+\n",
      "| id|category_1|category_2|category_1_index|category_2_index|category_2_OHE|\n",
      "+---+----------+----------+----------------+----------------+--------------+\n",
      "|  1|      L101|         R|             3.0|             0.0| (2,[0],[1.0])|\n",
      "|  2|      L201|         c|             4.0|             2.0|     (2,[],[])|\n",
      "|  3|      D111|         R|             1.0|             0.0| (2,[0],[1.0])|\n",
      "|  4|      F210|         R|             2.0|             0.0| (2,[0],[1.0])|\n",
      "|  5|      D110|         C|             0.0|             1.0| (2,[1],[1.0])|\n",
      "+---+----------+----------+----------------+----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#define stage 1 : transform the column category_1 to numeric\n",
    "stage_1 = StringIndexer(inputCol= 'category_1', outputCol='category_1_index')\n",
    "\n",
    "#define stage 2 : transform the column category_2 to numeric\n",
    "stage_2 = StringIndexer(inputCol='category_2', outputCol='category_2_index')\n",
    "\n",
    "#define stage 3 : one hot encode the numeric category_2 column\n",
    "stage_3 = OneHotEncoder(inputCols=['category_2_index'], outputCols=['category_2_OHE'])\n",
    "\n",
    "#setup the pipeline\n",
    "pipeline = Pipeline(stages=[stage_1, stage_2, stage_3])\n",
    "\n",
    "#fit the pipeline model and transform the data as defined\n",
    "pipeline_model = pipeline.fit(sample_df)\n",
    "sample_df_updated = pipeline_model.transform(sample_df)\n",
    "\n",
    "#view the transformed data\n",
    "sample_df_updated.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets take a more complex example of setting up a pipeline. Here we will do transformations on the data and build a logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this, we will create a sample dataframe which will be our training dataset with four features and the target label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+---------+-----+\n",
      "|feature_1|feature_2|feature_3|feature_4|label|\n",
      "+---------+---------+---------+---------+-----+\n",
      "|      2.0|        A|      S10|       40|  1.0|\n",
      "|      1.0|        X|      E10|       25|  1.0|\n",
      "|      4.0|        X|      S20|       10|  0.0|\n",
      "|      3.0|        Z|      S10|       20|  0.0|\n",
      "|      4.0|        A|      E10|       30|  1.0|\n",
      "|      2.0|        Z|      S10|       40|  0.0|\n",
      "|      5.0|        X|      D10|       10|  1.0|\n",
      "+---------+---------+---------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "#create a sample dataframe with 4 features and 1 label column\n",
    "sample_data_train = spark.createDataFrame([\n",
    "    (2.0, 'A', 'S10', 40, 1.0),\n",
    "    (1.0, 'X', 'E10', 25, 1.0),\n",
    "    (4.0, 'X', 'S20', 10, 0.0),\n",
    "    (3.0, 'Z', 'S10', 20, 0.0),\n",
    "    (4.0, 'A', 'E10', 30, 1.0),\n",
    "    (2.0, 'Z', 'S10', 40, 0.0),\n",
    "    (5.0, 'X', 'D10', 10, 1.0),\n",
    "], ['feature_1', 'feature_2', 'feature_3', 'feature_4', 'label'])\n",
    "\n",
    "#view the data\n",
    "sample_data_train.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, suppose this is the order of our pipeline:\n",
    "1. stage_1: Label Encode or String Index the column feature_2\n",
    "2. stage_2: Label Encode or String index the column features_3\n",
    "3. stage_3: One Hot Encode the indexed column of feature_2 and feature_3\n",
    "4. stage_4: create a vector of all the features required to train a Logistic Regression model\n",
    "5. stage_5: Build a Logistic Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to define the stages by providing the input column name and output column name. The final stage would be to build a logistic regression model. And in the end, when  we run the pipeline on the training dataset, it will run the steps in a sequence and add new columns to the dataframe(like rawPrediction, probability, and prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[2.0,0.0,1.0,1.0,...|  1.0|[-18.956871848873...|[5.84972108437796...|       1.0|\n",
      "|[1.0,1.0,0.0,0.0,...|  1.0|[-20.158269476976...|[1.75944137519465...|       1.0|\n",
      "|(7,[0,1,6],[4.0,1...|  0.0|[18.0148602858563...|[0.99999998499466...|       0.0|\n",
      "|(7,[0,3,6],[3.0,1...|  0.0|[24.5051237560211...|[0.99999999997721...|       0.0|\n",
      "|[4.0,0.0,1.0,0.0,...|  1.0|[-50.288624611182...|[1.44519958724398...|       1.0|\n",
      "|(7,[0,3,6],[2.0,1...|  0.0|[18.3280841853911...|[0.99999998902980...|       0.0|\n",
      "|[5.0,1.0,0.0,0.0,...|  1.0|[-17.986823547341...|[1.54319845459293...|       1.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#define stage 1: transform the column feature_2 to numeric\n",
    "stage_1 = StringIndexer(inputCol='feature_2', outputCol= 'feature_2_index')\n",
    "#define stage 2: transform the column feature_3 to numeric\n",
    "stage_2 = StringIndexer(inputCol='feature_3', outputCol= 'feature_3_index')\n",
    "#define stage 3: one hot encode the numeric versions of feature 2 and 3 generated from stage 1 and stage 2\n",
    "stage_3 = OneHotEncoder(inputCols=[stage_1.getOutputCol(), stage_2.getOutputCol()], outputCols= ['feature_2_encoded', 'feature_3_encoded'])\n",
    "#define stage 4: create a vector of all the features required to train the logistic regression model\n",
    "stage_4 = VectorAssembler(inputCols=['feature_1', 'feature_2_encoded', 'feature_3_encoded', 'feature_4'], outputCol='features')\n",
    "\n",
    "#define stage 5: logistic regression model\n",
    "stage_5 = LogisticRegression(featuresCol='features',labelCol='label')\n",
    "\n",
    "# Setup the pipeline\n",
    "regression_pipeline = Pipeline(stages=[stage_1, stage_2, stage_3, stage_4, stage_5])\n",
    "\n",
    "# fit the pipeline for the training data\n",
    "model = regression_pipeline.fit(sample_data_train)\n",
    "\n",
    "# transform the data\n",
    "sample_data_train = model.transform(sample_data_train)\n",
    "\n",
    "#view some of the columns generated\n",
    "sample_data_train.select('features', 'label', 'rawPrediction', 'probability', 'prediction').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "congrats! We have successfully set up the pipeline. Lets create a sample test dataset without the labels and this time, we do not need to define all the steps again.  We will just pass the data through the pipeline and we are done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+\n",
      "|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "|(7,[0,3,6],[3.0,1...|[21.9361235191363...|[0.99999999970265...|       0.0|\n",
      "|[1.0,1.0,0.0,0.0,...|[-19.516019417755...|[3.34426325212895...|       1.0|\n",
      "|(7,[0,2,6],[4.0,1...|[-22.297362790363...|[2.07194574533280...|       1.0|\n",
      "|[3.0,0.0,1.0,1.0,...|[-12.779832278243...|[2.81700837724663...|       1.0|\n",
      "|[4.0,1.0,0.0,0.0,...|[-24.163863117971...|[3.20455394170191...|       1.0|\n",
      "|(7,[0,4,6],[1.0,1...|[-22.543286459710...|[1.62022409523207...|       1.0|\n",
      "|[4.0,0.0,1.0,1.0,...|[-10.456293062940...|[2.87658445082073...|       1.0|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a sample data without the labels\n",
    "sample_data_test = spark.createDataFrame([\n",
    "    (3.0, 'Z', 'S10', 40),\n",
    "    (1.0, 'X', 'E10', 20),\n",
    "    (4.0, 'A', 'S20', 10),\n",
    "    (3.0, 'A', 'S10', 20),\n",
    "    (4.0, 'X', 'D10', 30),\n",
    "    (1.0, 'Z', 'E10', 20),\n",
    "    (4.0, 'A', 'S10', 30),\n",
    "], ['feature_1', 'feature_2', 'feature_3', 'feature_4'])\n",
    "\n",
    "#transform the data using the pipeline\n",
    "sample_data_test = model.transform(sample_data_test)\n",
    "\n",
    "#see the prediction on the test data\n",
    "sample_data_test.select('features', 'rawPrediction', 'probability', 'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
